# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MVZqCyFqMZ7gksGeTymVKvY1RqhtrtJT

#Handling data and performing data cleaning
"""

import pandas as pd

#loading the dataset in google colab
dataset_path = '/content/Stock Market-Historical Data of Top 10 Companies.csv'

# Loading the dataset into a pandas DataFrame
df = pd.read_csv(dataset_path)

# Displaying the initial state of the dataset
print("Initial dataset:")
print(df.head())
# Checking for missing values
missing_values = df.isnull().sum()
# Displaying the number of missing values in each column
print("\nMissing values in each column:")
print(missing_values)

# Handle missing values
df_cleaned = df.fillna(df.mean())

# Display the cleaned dataset
print("\nCleaned dataset:")
print(df_cleaned.head())

import pandas as pd

#loading the excel dataset
dataset_path = '/content/MF_Behavior.xlsx'

# Loading the dataset into a pandas DataFrame
df = pd.read_excel(dataset_path)

# Displaying the initial state of the dataset
print("Initial dataset:")
print(df.head())

# Checking for missing values
missing_values = df.isnull().sum()

# Displaying the number of missing values in each column
print("\nMissing values in each column:")
print(missing_values)

# Handling missing values (for simplicity, filling missing values with the mean)
df_cleaned = df.fillna(df.mean())

# Displaying the cleaned dataset
print("\nCleaned dataset:")
print(df_cleaned.head())

"""#Reading the columns of the datasets"""

import pandas as pd

# Reading the columns of the Stock Market dataset
stock_data = pd.read_csv('/content/Stock Market-Historical Data of Top 10 Companies.csv')
print("Columns of Stock Market dataset:")
print(stock_data.columns)

# Reading the columns of the MF_Behavior dataset
behavior_data = pd.read_excel('/content/MF_Behavior.xlsx')
print("\nColumns of MF_Behavior dataset:")
print(behavior_data.columns)

"""# Random forest regression model"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_excel('/content/MF_Behavior.xlsx')
df = df.set_index('Investor_ID')
X = df.drop(['AUM'], axis=1)
y = df['AUM']
# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Build and train a random forest regression model
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)
# Predict on test data
y_pred = rf_model.predict(X_test)
# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse}')
# plots
plt.plot(y_test.index, y_test, label='Actual AUM')
plt.plot(y_test.index, y_pred, label='Predicted AUM')
plt.legend()
plt.show()

"""#Regression Model for Correlation Studies:"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
# Load the dataset
df = pd.read_excel('/content/MF_Behavior.xlsx')
df = df.set_index('Investor_ID')
# Assuming you want to predict 'AUM' based on other features
# Replace 'AUM' with the target variable you are interested in
X = df.drop(['AUM'], axis=1)
y = df['AUM']
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the linear regression model
model = LinearRegression()

# Training the model
model.fit(X_train, y_train)

# Predicting on the test set
y_pred = model.predict(X_test)

# Evaluating the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(f'Mean Squared Error: {mse}')
print(f'R^2 Score: {r2}')
# Visualization of the predicted vs. actual values
plt.scatter(y_test, y_pred)
plt.xlabel('Actual AUM')
plt.ylabel('Predicted AUM')
plt.title('Actual vs. Predicted AUM')
plt.show()

"""#Time series analysis and hypothesis testing"""

# Selecting features and target variable
features = ['Longevity', 'Female', 'Age', 'Income', 'ProfManage', 'Diversification',
            'Affordability', 'Liquidity', 'Growth', 'Trustworthiness', 'Technology',
            'Integrity', 'BrandValue']
target = 'AUM'
X = df[features]
y = df[target]
# Splitting the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Training a linear regression model using scikit-learn
model = LinearRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
# Displaying model evaluation results
print(f'Mean Squared Error: {mse}')
coefficients = pd.DataFrame({'Feature': features, 'Coefficient': model.coef_})
print(coefficients)
# Hypothesis testing using statsmodels
X_train_sm = sm.add_constant(X_train)
ols_model = sm.OLS(y_train, X_train_sm).fit()
print(ols_model.summary())

plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred)
plt.xlabel('Actual AUM')
plt.ylabel('Predicted AUM')
plt.title('Predicted vs. Actual AUM')
plt.show()
# Plotting histogram of residuals
residuals = y_test - y_pred
plt.figure(figsize=(10, 6))
sns.histplot(residuals, kde=True)
plt.xlabel('Residuals')
plt.ylabel('Frequency')
plt.title('Histogram of Residuals')
plt.show()
# Bar chart of coefficients
plt.figure(figsize=(12, 8))
sns.barplot(x='Coefficient', y='Feature', data=coefficients)
plt.title('Linear Regression Coefficients')
plt.xlabel('Coefficient Value')
plt.ylabel('Feature')
plt.show()

"""#ADDED MODELS

#Support Vector Machines (SVM)

---
"""

X = df.drop(['AUM'], axis=1)
y = df['AUM']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
model = SVR(kernel='linear')
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(f'Mean Squared Error: {mse}')
print(f'R^2 Score: {r2}')
plt.scatter(y_test, y_pred)
plt.xlabel('Actual AUM')
plt.ylabel('Predicted AUM')
plt.title('Actual vs. Predicted AUM (SVM Regression)')
plt.show()

"""# Gaussian Process Regresssion"""

# Selecting features and target variable
features = ['Longevity', 'Female', 'Age', 'Income', 'ProfManage', 'Diversification',
            'Affordability', 'Liquidity', 'Growth', 'Trustworthiness', 'Technology',
            'Integrity', 'BrandValue']
target = 'AUM'
X = df[features]
y = df[target]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
kernel = C(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-2, 1e2))
model = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10, random_state=42)
model.fit(X_train, y_train)
y_pred, sigma = model.predict(X_test, return_std=True)
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse}')
print("\nRegression Coefficients:")
for feature, coef in zip(features, model.kernel_.theta[:-1]):
    print(f"{feature}: {coef}")
print("\nFeature Importance:")
feature_importance = np.abs(model.kernel_.theta[:-1])
feature_importance /= feature_importance.sum()
for feature, importance in zip(features, feature_importance):
    print(f"{feature}: {importance}")
plt.figure(figsize=(10, 6))
plt.errorbar(y_test, y_pred, yerr=1.96 * sigma, fmt='o', ecolor='red', markersize=8, alpha=0.7)
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], '--', color='gray')
plt.xlabel('Actual AUM')
plt.ylabel('Predicted AUM')
plt.title('Actual vs. Predicted AUM (Gaussian Process Regression)')
plt.show()

"""#Naive Bayes Regresssion"""

features = ['Longevity', 'Female', 'Age', 'Income', 'ProfManage', 'Diversification',
            'Affordability', 'Liquidity', 'Growth', 'Trustworthiness', 'Technology',
            'Integrity', 'BrandValue']
target = 'AUM'
X = df[features]
y = df[target]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
model = GaussianNB()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse}')
plt.scatter(y_test, y_pred)
plt.xlabel('Actual AUM')
plt.ylabel('Predicted AUM')
plt.title('Actual vs. Predicted AUM (Gaussian Naive Bayes Regression)')
plt.show()

"""#Decision Tree Regression"""

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
model = DecisionTreeRegressor(random_state=42)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
feature_importance = pd.DataFrame({'Feature': features, 'Importance': model.feature_importances_})
feature_importance = feature_importance.sort_values(by='Importance', ascending=False)
print("\nFeature Importance:")
print(feature_importance)
plt.figure(figsize=(15, 8))
plot_tree(model, feature_names=features, filled=True, rounded=True, fontsize=10)
plt.title('Decision Tree Visualization')
plt.show()
# Summary of the Analysis
print(f'\nMean Squared Error: {mse}')
Plots
plt.xlabel('Actual AUM')
plt.ylabel('Predicted AUM')
plt.title('Actual vs. Predicted AUM (Decision Tree Regression)')
plt.show()